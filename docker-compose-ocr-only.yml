services:
  # api-gateway:
  #   build:
  #     context: api-gateway
  #     dockerfile: Dockerfile
  #   ports:
  #     - "8200:80"

  api-ocr:
    build:
      context: api-ocr
      dockerfile: Dockerfile
    command: ["uv", "run", "main_api_ocr.py"]
    ports:
      - "8204:7860"
    environment:
      - GRADIO_SERVER_PORT=7860
      - GRADIO_SERVER_NAME=0.0.0.0
      - CELERY_BROKER_URL=amqp://guest:guest@rabbitmq:5672
      - CELERY_RESULT_BACKEND=rpc://
      - GRADIO_CONCURRENCY_LIMIT=10
      # FIXME check this backend works across multiple machines
    depends_on:
      - rabbitmq


  ocr-worker:
    build:
      context: ocr-worker
      dockerfile: test.dockerfile
      # dockerfile: Dockerfile
    command: [uv, run, celery, --app=worker.celery, worker, --concurrency=1, -P, threads, --loglevel=INFO]
    environment: 
      - CELERY_BROKER_URL=amqp://guest:guest@rabbitmq:5672
      - CELERY_RESULT_BACKEND=rpc://
    depends_on:
      - rabbitmq
    # volumes:
      # - pero-cache:/app/pero-models

## â†“ TODO use a volume but add a startup script to download the models if needed
## TODO add 1-time service (task) for ocr models download
## Use a shared volume to store them; downloading them only if they are not already there
## use an environment variable to set the path in the worker service


  rabbitmq:
    image: rabbitmq:3.7.8

  flower:
    image: mher/flower
    ports:
      - "5555:5555"
    environment:
      - CELERY_BROKER_URL=amqp://guest:guest@rabbitmq:5672
    depends_on:
      - rabbitmq

## TODO result backend for celery?


# volumes:
  # pero-cache: